{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining files together\n",
    "\n",
    "The purpose of this file is to convey methodology and logic that can be used to join our files together.  To this end, we will cover each our of Raw 2.0 files using a slice of the full job records.  These methodologies can be scaled up in a SQL data warehouse or by chunking the files depending on the subset you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tarfile\n",
    "import sqlite3\n",
    "from xml.etree import ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Job Records\n",
    "\n",
    "### Job records is the core table.  Everything will be joined into Job Records.\n",
    "\n",
    "##### I will only be using a small subset of the columns for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading raw sample slice\n",
    "with tarfile.open('raw-sample.tar.gz', \"r:*\") as tar:\n",
    "    # Get path to the job_records file within the tarfile\n",
    "    csv_path = tar.getnames()[1]\n",
    "    # Load job records file into pandas dataframe\n",
    "    job_records = pd.read_csv(tar.extractfile(csv_path),\n",
    "                    parse_dates = ['created','delete_date'],\n",
    "                    low_memory = False,\n",
    "                    usecols = ['hash','title','company_id','created','delete_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join fs_company_reference\n",
    "\n",
    "##### I am only joining primary information, and only based on created date as that is a good starting point.  Please reach out to Linkup if you would like to discuss options for joining to use information on multiple dates to account for jobs that span changes/M&A activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read PIT Ticker File\n",
    "FS_reference = pd.read_csv('fs_company_reference_daily_2019-11-13.csv.gz',\n",
    "                        parse_dates = ['start_date','end_date'])\n",
    "                      \n",
    "# Filter for only primary exchange and columns needed for join\n",
    "FS_reference = FS_reference[FS_reference.primary_flag == True]\n",
    "\n",
    "# Formatting Timestamps for Merge and dealing with missing values\n",
    "FS_reference['start_date'] = FS_reference['start_date'].fillna(str(job_records.created.min().date()))\n",
    "FS_reference['start_date'] = pd.to_datetime(FS_reference['start_date'])\n",
    "FS_reference['end_date'] = FS_reference['end_date'].fillna(str(job_records.created.max().date()))\n",
    "FS_reference['end_date'] = pd.to_datetime(FS_reference['end_date'])\n",
    "\n",
    "# Make the db in memory\n",
    "conn = sqlite3.connect(':memory:')\n",
    "\n",
    "# Write the tables\n",
    "job_records.to_sql('Job_Records', conn, index=False)\n",
    "FS_reference.to_sql('FS_reference', conn, index=False)\n",
    "\n",
    "# Query and create new joined table\n",
    "qry = '''\n",
    "    SELECT Job_Records.*,\n",
    "    FS_reference.stock_ticker,\n",
    "    FS_reference.stock_exchange_country,\n",
    "    FS_reference.stock_exchange_name,\n",
    "    FS_reference.primary_flag\n",
    "    \n",
    "    FROM Job_Records\n",
    "    \n",
    "    LEFT JOIN FS_reference\n",
    "    ON (Job_Records.created between FS_reference.start_date and FS_reference.end_date and\n",
    "       Job_Records.company_id = FS_reference.company_id);\n",
    "    '''\n",
    "job_records_joined = pd.read_sql_query(qry, conn)\n",
    "\n",
    "FS_reference = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join PIT Company Reference\n",
    "\n",
    "##### For the PIT Company Reference File, most use-cases only require joining the most current company information to all records historically.  If you would like to use the PIT nature of the file, please reach out to Linkup and we would be happy to provide assistance with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIT_Company_Reference = pd.read_csv('raw_pit_company_reference_full_2019-11-16.csv.gz')\n",
    "\n",
    "# Filter Company Reference file to include only the companies that we pulled\n",
    "#PIT_Company_Reference = PIT_Company_Reference[PIT_Company_Reference.company_id.isin(job_records.company_id.unique())]\n",
    "\n",
    "# Filter Company Reference for the latest Information\n",
    "#PIT_Company_Reference = PIT_Company_Reference[PIT_Company_Reference.end_date.isnull()]\n",
    "\n",
    "# Make the db in memory\n",
    "conn = sqlite3.connect(':memory:')\n",
    "\n",
    "# Write the tables\n",
    "job_records.to_sql('Job_Records', conn, index=False)\n",
    "PIT_Company_Reference.to_sql('PIT_Company_Reference', conn, index=False)\n",
    "\n",
    "qry = '''\n",
    "    SELECT \n",
    "        Job_Records.*, \n",
    "        PIT_Company_Reference.company_url,\n",
    "        PIT_Company_Reference.lei,\n",
    "        PIT_Company_Reference.open_perm_id\n",
    "        \n",
    "    FROM Job_Records\n",
    "    LEFT JOIN PIT_Company_Reference\n",
    "    ON Job_Records.company_id = PIT_Company_Reference.company_id and\n",
    "        Job_Records.created BETWEEN \n",
    "            PIT_Company_Reference.start_date and \n",
    "            PIT_Company_Reference.end_date\n",
    "        ;\n",
    "'''\n",
    "\n",
    "job_records_joined = pd.read_sql_query(qry, conn)\n",
    "\n",
    "PIT_Company_Reference = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Scrape Log\n",
    "\n",
    "##### Here I will do a basic join to show the date of the next scrape change.  The primary purpose of this is so that if you see outliers, you can see when the next code change was.  If it was shortly after the outlier, the outlier is likely a scrape break vs a true signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Scrape Log\n",
    "Scrape_Log = pd.read_csv('raw_company_scrape_log_full_2019-11-01.csv')\n",
    "\n",
    "# Filter Scrape Log to only include code changes for the companies that we pulled\n",
    "Scrape_Log = Scrape_Log[Scrape_Log.scrape_changed == True]\n",
    "Scrape_Log = Scrape_Log[Scrape_Log.company_id.isin(job_records.company_id.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift Date for SQL query convenience\n",
    "Scrape_Log['date_shifted'] = Scrape_Log.groupby(['company_id'])['date'].shift(1)\n",
    "Scrape_Log['date_shifted'] = Scrape_Log['date_shifted'].fillna('2007-01-01')\n",
    "\n",
    "Scrape_Log['date_shifted'] = pd.to_datetime(Scrape_Log['date_shifted'])\n",
    "Scrape_Log['date'] = pd.to_datetime(Scrape_Log['date'])\n",
    "# Make the db in memory\n",
    "conn = sqlite3.connect(':memory:')\n",
    "\n",
    "# Write the tables\n",
    "job_records.to_sql('Job_Records', conn, index=False)\n",
    "Scrape_Log.to_sql('Scrape_Log', conn, index=False)\n",
    "\n",
    "# Query and create new joined table\n",
    "qry = '''\n",
    "    SELECT Job_Records.*,\n",
    "    Scrape_Log.date as Next_Scrape_Change\n",
    "    \n",
    "    FROM Job_Records\n",
    "    \n",
    "    LEFT JOIN Scrape_Log\n",
    "    ON (Job_Records.created between Scrape_Log.date_shifted and Scrape_Log.date and\n",
    "       Job_Records.company_id = Scrape_Log.company_id);\n",
    "    '''\n",
    "job_records_joined = pd.read_sql_query(qry, conn)\n",
    "\n",
    "Scrape_Log = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Load Descriptions file\n",
    "Job_Descriptions = pd.DataFrame(\n",
    "        list(map(lambda x: (x[0].text,x[1].text),\n",
    "                 ET.parse('../raw-sample/raw-sample-descriptions.xml').getroot())), \n",
    "'''    \n",
    "with tarfile.open('raw-sample.tar.gz', \"r:*\") as tar:\n",
    "    # Get path to the job_records file within the tarfile\n",
    "    csv_path = tar.getnames()[0]\n",
    "    # Load job records file into pandas dataframe\n",
    "    #job_records = pd.read_csv(tar.extractfile(csv_path),\n",
    "    #    columns = ['job_hash','description'])\n",
    "\n",
    "    Job_Descriptions = pd.DataFrame(\n",
    "        list(map(lambda x: (x[0].text,x[1].text),\n",
    "                 ET.parse(tar.extractfile(csv_path)).getroot())),\n",
    "        columns = ['job_hash','description'])\n",
    "# Make the db in memory\n",
    "conn = sqlite3.connect(':memory:')\n",
    "\n",
    "# Write the tables\n",
    "job_records.to_sql('Job_Records', conn, index=False)\n",
    "Job_Descriptions.to_sql('Job_Descriptions', conn, index=False)\n",
    "\n",
    "qry = '''\n",
    "    SELECT \n",
    "        Job_Records.*, \n",
    "        Job_Descriptions.description\n",
    "    \n",
    "    \n",
    "    FROM Job_Records\n",
    "    LEFT JOIN Job_Descriptions\n",
    "    ON (Job_Records.hash = Job_Descriptions.job_hash);\n",
    "'''\n",
    "job_records_joined = pd.read_sql_query(qry, conn)\n",
    "\n",
    "Job_Descriptions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Results of Big Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>title</th>\n",
       "      <th>company_id</th>\n",
       "      <th>created</th>\n",
       "      <th>delete_date</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0234422f9c5eff7f6d1d008c3c31dae6</td>\n",
       "      <td>UI Artist, Double Helix Games, Amazon Game Stu...</td>\n",
       "      <td>469</td>\n",
       "      <td>2015-06-20 02:39:07+00:00</td>\n",
       "      <td>2015-09-18 07:51:10+00:00</td>\n",
       "      <td>Amazon is all in on games.\\n\\nWe believe the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0c392d37eb6dfb15b1ffc2a48b8d95e7</td>\n",
       "      <td>Warehouse Team Member (Seasonal, Part Time, Fl...</td>\n",
       "      <td>469</td>\n",
       "      <td>2019-01-15 16:06:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>Shifts:\\n\\nOver-night, Early Morning, Day, Eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0ee6d7263a67121e8405af8ad8fdef2a</td>\n",
       "      <td>Responsable des opérations de contrôle d'inven...</td>\n",
       "      <td>469</td>\n",
       "      <td>2018-04-17 15:04:00+00:00</td>\n",
       "      <td>2018-04-18 15:05:00+00:00</td>\n",
       "      <td>ous aimez l'action ?\\n\\nVous aimeriez travaill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0aaf97d36a5279ac18a43d1e34344232</td>\n",
       "      <td>Relationship Manager-Newton/Waltham, MA Area</td>\n",
       "      <td>381</td>\n",
       "      <td>2015-10-15 20:58:31+00:00</td>\n",
       "      <td>2015-10-23 06:54:04+00:00</td>\n",
       "      <td>Located in a banking center, Relationship Mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>05e060a4db459d502ed71fc78ea75f62</td>\n",
       "      <td>Database Administrator II - AMZ1781</td>\n",
       "      <td>469</td>\n",
       "      <td>2017-07-02 09:46:00+00:00</td>\n",
       "      <td>2017-07-22 19:00:00+00:00</td>\n",
       "      <td>MULTIPLE POSITIONS AVAILABLE Company: Amazon C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015682</td>\n",
       "      <td>fce9446a1c82c043b885ec53bd7c79e9</td>\n",
       "      <td>Client Service Representative</td>\n",
       "      <td>381</td>\n",
       "      <td>2017-10-10 00:03:00+00:00</td>\n",
       "      <td>2017-10-17 04:16:56+00:00</td>\n",
       "      <td>Job Description:\\n\\nFinancial Center Client Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015683</td>\n",
       "      <td>f69454c25df4c853541eecdfc4a3d0ef</td>\n",
       "      <td>DCO Lead(Away)</td>\n",
       "      <td>469</td>\n",
       "      <td>2017-11-08 16:05:00+00:00</td>\n",
       "      <td>2017-11-15 16:18:17+00:00</td>\n",
       "      <td>Are you passionate about finding process impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015684</td>\n",
       "      <td>f1399ba8d1ae6adddad701beb823a8f8</td>\n",
       "      <td>Director of Inclusion and Diversity - Minneton...</td>\n",
       "      <td>383</td>\n",
       "      <td>2017-07-25 22:08:00+00:00</td>\n",
       "      <td>2017-08-29 15:50:44+00:00</td>\n",
       "      <td>Here, you'll help attract, lead, support and r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015685</td>\n",
       "      <td>f6a9ba8e6c257788e5599388f770dfd0</td>\n",
       "      <td>Registered Client Associate</td>\n",
       "      <td>381</td>\n",
       "      <td>2016-08-26 16:26:37+00:00</td>\n",
       "      <td>2016-08-30 17:50:11+00:00</td>\n",
       "      <td>Business Overview\\n\\nMerrill Lynch Wealth Mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015687</td>\n",
       "      <td>f69d7af8bcd1b2295781bc20bc662871</td>\n",
       "      <td>Strategic Customer Engagement Specialist, Korea</td>\n",
       "      <td>469</td>\n",
       "      <td>2018-05-22 15:04:00+00:00</td>\n",
       "      <td>2019-02-13 16:09:00+00:00</td>\n",
       "      <td>Do you have a knack for analyzing business dea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>607423 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     hash  \\\n",
       "2        0234422f9c5eff7f6d1d008c3c31dae6   \n",
       "7        0c392d37eb6dfb15b1ffc2a48b8d95e7   \n",
       "8        0ee6d7263a67121e8405af8ad8fdef2a   \n",
       "9        0aaf97d36a5279ac18a43d1e34344232   \n",
       "13       05e060a4db459d502ed71fc78ea75f62   \n",
       "...                                   ...   \n",
       "1015682  fce9446a1c82c043b885ec53bd7c79e9   \n",
       "1015683  f69454c25df4c853541eecdfc4a3d0ef   \n",
       "1015684  f1399ba8d1ae6adddad701beb823a8f8   \n",
       "1015685  f6a9ba8e6c257788e5599388f770dfd0   \n",
       "1015687  f69d7af8bcd1b2295781bc20bc662871   \n",
       "\n",
       "                                                     title  company_id  \\\n",
       "2        UI Artist, Double Helix Games, Amazon Game Stu...         469   \n",
       "7        Warehouse Team Member (Seasonal, Part Time, Fl...         469   \n",
       "8        Responsable des opérations de contrôle d'inven...         469   \n",
       "9             Relationship Manager-Newton/Waltham, MA Area         381   \n",
       "13                     Database Administrator II - AMZ1781         469   \n",
       "...                                                    ...         ...   \n",
       "1015682                      Client Service Representative         381   \n",
       "1015683                                     DCO Lead(Away)         469   \n",
       "1015684  Director of Inclusion and Diversity - Minneton...         383   \n",
       "1015685                        Registered Client Associate         381   \n",
       "1015687    Strategic Customer Engagement Specialist, Korea         469   \n",
       "\n",
       "                           created                delete_date  \\\n",
       "2        2015-06-20 02:39:07+00:00  2015-09-18 07:51:10+00:00   \n",
       "7        2019-01-15 16:06:00+00:00                       None   \n",
       "8        2018-04-17 15:04:00+00:00  2018-04-18 15:05:00+00:00   \n",
       "9        2015-10-15 20:58:31+00:00  2015-10-23 06:54:04+00:00   \n",
       "13       2017-07-02 09:46:00+00:00  2017-07-22 19:00:00+00:00   \n",
       "...                            ...                        ...   \n",
       "1015682  2017-10-10 00:03:00+00:00  2017-10-17 04:16:56+00:00   \n",
       "1015683  2017-11-08 16:05:00+00:00  2017-11-15 16:18:17+00:00   \n",
       "1015684  2017-07-25 22:08:00+00:00  2017-08-29 15:50:44+00:00   \n",
       "1015685  2016-08-26 16:26:37+00:00  2016-08-30 17:50:11+00:00   \n",
       "1015687  2018-05-22 15:04:00+00:00  2019-02-13 16:09:00+00:00   \n",
       "\n",
       "                                               description  \n",
       "2        Amazon is all in on games.\\n\\nWe believe the e...  \n",
       "7        Shifts:\\n\\nOver-night, Early Morning, Day, Eve...  \n",
       "8        ous aimez l'action ?\\n\\nVous aimeriez travaill...  \n",
       "9        Located in a banking center, Relationship Mana...  \n",
       "13       MULTIPLE POSITIONS AVAILABLE Company: Amazon C...  \n",
       "...                                                    ...  \n",
       "1015682  Job Description:\\n\\nFinancial Center Client Se...  \n",
       "1015683  Are you passionate about finding process impro...  \n",
       "1015684  Here, you'll help attract, lead, support and r...  \n",
       "1015685  Business Overview\\n\\nMerrill Lynch Wealth Mana...  \n",
       "1015687  Do you have a knack for analyzing business dea...  \n",
       "\n",
       "[607423 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_records_joined[~job_records_joined.description.isnull()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
