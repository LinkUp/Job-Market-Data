{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Description Analysis ('Bag-of-Words') Single Word Approach\n",
    "\n",
    "##### This guide is to communicate an approach or methodology for analyzing job descriptions. These are several potential options that this can be used to:¶\n",
    "+ Compare technologies being used by companies in the same sector or companies operating as competitors.\n",
    "+ Analyze the number of companies that are hiring for specific technologies to gain an understaing of 'market share'. For example: how many companies are hiring for aws vs azure?\n",
    "+ Analyze specific companies that are hiring for technologies that are not being widely used.\n",
    "+ Identify trending technologies; identify companies as early adopters or technology laggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# set stopwords.  After installing the nltk library, you can modify the stopwords text file to add or \n",
    "# remove additional stop words as desired.  For this tutorial, I am using the library defaults.\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Visual control for the purpose of tutorial display\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Specific job Descriptions You Are Looking to Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Get hashes for the records you want to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get Hashes you are interested in analyzing.  This is done by querying job_records.  For example, if using SQL you can use the following Query.\n",
    "```mysql\n",
    "SELECT job_records.hash\n",
    "FROM job_records\n",
    "JOIN fs_company_identifiers ON job_records.company_id = fs_company_identifiers.company_id\n",
    "WHERE \n",
    "    fs_company_identifiers.primary_flag = True AND \n",
    "    fs_company_identifiers.end_date IS NULL AND isin IN ('US0378331005', 'US5949181045', 'US0231351067');\n",
    "\n",
    "INTO OUTFILE '/hashes.csv'\n",
    "FIELDS TERMINATED BY ','\n",
    "ENCLOSED BY '\"'\n",
    "LINES TERMINATED BY '\\n';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load job descriptions into a database and filter for hashes you are interested in.¶\n",
    "\n",
    "##### Caution: We recommend using a production grade database of some kind for instead of flat csv files for production. This guide is only to communicate methodology.¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This converts xml descriptions to csv.  I will be working with descriptions locally as flat csv files  \n",
    "# It is highly reccomended that you replace this step by loading data into a database based on your internal systems\n",
    "# SQL, SolR, mongodb, etc.\n",
    "\n",
    "files = os.listdir(\"../linkup_job_descriptions/xml/\")\n",
    "\n",
    "for file in files:        \n",
    "    df = pd.DataFrame(list(map(lambda x: (x[0].text,x[1].text),ET.parse(file).getroot())))\n",
    "    df.columns = ['jobHash','jobDesc']\n",
    "    \n",
    "    # Filter for hashes you want from your universe\n",
    "    df = df[df['jobHash'].isin(hashes)]\n",
    "    \n",
    "    # Ensure all jobDesc are type string\n",
    "    df['jobDesc'] = df['jobDesc'].astype(str)\n",
    "    \n",
    "    # Text cleanup\n",
    "    df['jobDesc'] = df['jobDesc'].str.replace('\\n',\" \").str.replace('\\nn',\" \").\n",
    "    df['jobDesc'] = df['jobDesc'].str.translate(str.maketrans('','',string.punctuation)).str.lower().str.split()\n",
    "    df['jobDesc'] = df['jobDesc'].apply(lambda x: list(dict.fromkeys(x)))\n",
    "    \n",
    "    # Remove Stopwords\n",
    "    df['jobDesc'] = df['jobDesc'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    df['jobDesc'] = df['jobDesc'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Export\n",
    "    df.to_csv(\"../linkup_job_descriptions/csv/\"+str(x) + '-' + file, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Combine all output files from previous step and do word counts with threshold\n",
    "\n",
    "##### Caution: We recommend using a production grade database of some kind for instead of flat csv files for production. This guide is only to communicate methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"../linkup_job_descriptions/csv/\")\n",
    "\n",
    "# Combine all csv files together into one DataFrame\n",
    "df_full = pd.DataFrame()    \n",
    "for file in files:\n",
    "    temp = pd.read_csv(file)\n",
    "    df_full = pd.concat([df_full,temp])\n",
    "\n",
    "# Count the number of job descriptions that each word appears in\n",
    "df_full['jobDesc'] = df_full['jobDesc'].astype(str)  \n",
    "counts = collections.Counter([y for x in df_full['jobDesc'].values.flatten() for y in x.split()])    \n",
    "\n",
    "# Filter out job descriptions based on min_threshhold.  This should be adjusted based on use case.\n",
    "min_threshold = 1000\n",
    "counts = {x : counts[x] for x in counts if counts[x] >= min_threshold\n",
    "          \n",
    "# Optional conversion to DataFrame - Can be left as a Dictionary.\n",
    "counts = pd.DataFrame(list(counts.items()))\n",
    "counts.to_csv('counts.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Examine Results\n",
    "\n",
    "##### Please see below for this methodology being applied to the Russell 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Results\n",
    "counts = pd.read_csv('/Users/iflath/DataFiles/raw_daily_2019-10-24/Russel 3k/counts.csv', names = ['word','count'])\n",
    "counts = counts.sort_values(['count'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1557</td>\n",
       "      <td>data</td>\n",
       "      <td>203320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>analytics</td>\n",
       "      <td>42682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3313</td>\n",
       "      <td>sql</td>\n",
       "      <td>26255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1571</td>\n",
       "      <td>python</td>\n",
       "      <td>23195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1592</td>\n",
       "      <td>aws</td>\n",
       "      <td>18327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3103</td>\n",
       "      <td>javascript</td>\n",
       "      <td>12073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6491</td>\n",
       "      <td>azure</td>\n",
       "      <td>8843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7342</td>\n",
       "      <td>r</td>\n",
       "      <td>6957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5815</td>\n",
       "      <td>mongodb</td>\n",
       "      <td>2136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word   count\n",
       "1557        data  203320\n",
       "3760   analytics   42682\n",
       "3313         sql   26255\n",
       "1571      python   23195\n",
       "1592         aws   18327\n",
       "3103  javascript   12073\n",
       "6491       azure    8843\n",
       "7342           r    6957\n",
       "5815     mongodb    2136"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for specific key words\n",
    "search_words = ['python','r','data','analytics','javascript','mongodb','sql','aws','azure']\n",
    "counts[counts['word'].isin(search_words)].sort_values(['count'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Method for Parsing descriptions that will be faster for smaller subsets, or small subsets of key words.\n",
    "\n",
    "##### This Command Line script will do the following:\n",
    "\n",
    "1. Loop line by line through xml file\n",
    "2. Use regular expressions to grab the hash and the description\n",
    "3. Search each description for specific keyword\n",
    "\n",
    "##### This is intended to be started code for fast processing and analysis of the xml job descriptions file, not final production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, re, csv\n",
    "import argparse,sys\n",
    "\n",
    "##Add CLI Args\n",
    "parser=argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--inputFile', help='the XML descriptions file to read')\n",
    "parser.add_argument('--outputFile', help='the CSV file to print the filtered descriptions')\n",
    "parser.add_argument('--keyword', help='the keyword to search descriptions for')\n",
    "\n",
    "args=parser.parse_args()\n",
    "\n",
    "if (not args.inputFile or not args.outputFile):\n",
    "    print(\"Must specify both an input and output file.\")\n",
    "    exit()\n",
    "\n",
    "if (args.keyword):\n",
    "    keyword = args.keyword\n",
    "\n",
    "start = time.time()\n",
    "count = 0;\n",
    "with open(args.inputFile) as file, open(args.outputFile, \"wb\") as outFile:\n",
    "    jobHash = '';\n",
    "    jobDesc = '';\n",
    "    inJobElement = False\n",
    "    writer = csv.writer(outFile)\n",
    "    for line in file:\n",
    "        if \"<job>\" in line and not \"</job>\" in line:\n",
    "            inJobElement = True\n",
    "        elif \"</job>\" in line:\n",
    "            inJobElement = False\n",
    "            if \"<job>\" in line:\n",
    "                inJobElement = True\n",
    "        elif inJobElement == True:\n",
    "            if \"<hash>\" in line:\n",
    "\n",
    "                jobHash = re.match(r\"\\s*<hash>(.*)<\", line).group(1)\n",
    "            \n",
    "            elif \"<description>\" in line:\n",
    "\n",
    "                jobDesc = re.match(r\"\\s*<description><..CDATA.(.*)]]><\", line).group(1)\n",
    "                \n",
    "        if jobHash and jobDesc:\n",
    "            #do Processing\n",
    "            if(not keyword or keyword in jobDesc):\n",
    "                writer.writerow([jobHash, jobDesc])\n",
    "                jobHash = '';\n",
    "                jobDesc = '';\n",
    "                count = count + 1\n",
    "        \n",
    "print count\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
